{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_correction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Kq_4Wn6WaSMs",
        "iglj43P5aaaO",
        "a01q0ScoaljM",
        "Giz30N5Gasy3"
      ],
      "authorship_tag": "ABX9TyMUbs0S+ML4yHfwzzzlZNuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedaliELfeky/Spelling-Corrector/blob/main/Word_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports "
      ],
      "metadata": {
        "id": "YrZshzP7aEiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from requests import get\n",
        "from termcolor import colored\n"
      ],
      "metadata": {
        "id": "nyg6fZ55aLb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load bigdata\n",
        "\n",
        "p = 'https://norvig.com/big.txt'\n",
        "page_rquest = get(p).text\n",
        "with open('big.txt', 'w') as f:\n",
        "    f.writelines(page_rquest)\n"
      ],
      "metadata": {
        "id": "dMeun5tqolTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load common errors\n",
        "\n",
        "misspelling_file = get(\"https://norvig.com/ngrams/spell-errors.txt\").text\n",
        "misspelling_file = misspelling_file.lower()\n",
        "with open(\"misspelling.txt\", 'w') as f:\n",
        "    f.writelines(misspelling_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "YeSIzpWbKs4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test data\n",
        "\n",
        "testCases = {'spell-testset1.txt':\"https://norvig.com/spell-testset1.txt\",\n",
        "             'spell-testset2.txt':\"https://norvig.com/spell-testset1.txt\"}\n",
        "\n",
        "for i in testCases:\n",
        "    with open(i, 'w') as f:\n",
        "        f.writelines(get(testCases[i]).text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IkjirGNQPcWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "5_UJCFbWaN6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABLCE_SMOOTHING = 1e-5\n",
        "\n",
        "VOCAB_PATH = \"big.txt\"\n"
      ],
      "metadata": {
        "id": "mSmrG3S_aNkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions "
      ],
      "metadata": {
        "id": "Kq_4Wn6WaSMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiply dictionaries of probability"
      ],
      "metadata": {
        "id": "iglj43P5aaaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mul_dict(x:dict, y:dict):\n",
        "    \"\"\"\n",
        "    Multiply probability of 2 dictionary\n",
        "    Input:\n",
        "        x(dict): all vocab probabilty that similar to the word that need correction\n",
        "        y(dict): all vocab probabilty that apeared in the same context\n",
        "\n",
        "    Output:\n",
        "        z(dict): sorted dictionary of the liklihood probability\n",
        "    \"\"\"\n",
        "    for i in x:\n",
        "        if i in y:\n",
        "            x[i] *= y[i]\n",
        "        else:\n",
        "            # if the word didn't appear in the context we will reduce its probabilty\n",
        "            x[i] *= LABLCE_SMOOTHING\n",
        "\n",
        "    z = list(sorted(x.items(), key=lambda x: x[1]))\n",
        "    return dict(reversed(z))\n",
        "                    "
      ],
      "metadata": {
        "id": "zAqEHFFpaXaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate regex expression"
      ],
      "metadata": {
        "id": "a01q0ScoaljM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_exp(w1=None, w2=None):\n",
        "    \"\"\"\n",
        "    Generates Regex expressions\n",
        "    Input\n",
        "        w1: starting word for the search\n",
        "        w2: ending word for the search\n",
        "    Output\n",
        "        pattern: regex pattern \n",
        "    \"\"\"\n",
        "    if w1 and w2:\n",
        "        return rf\"{w1} \\w+ {w2}\"\n",
        "    elif w1:\n",
        "        return rf'{w1} \\w+\\b'\n",
        "    elif w2:\n",
        "        return rf'\\w+ {w2}'\n",
        "    \n",
        "    return r'[a-zA-Z]+'"
      ],
      "metadata": {
        "id": "L4zetdRraqIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reading and processing data"
      ],
      "metadata": {
        "id": "Giz30N5Gasy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_misspelled(file_path='misspelling.txt'):\n",
        "    \"\"\"\n",
        "    Read the misspelled file\n",
        "    Input:\n",
        "        file_path(str): string contains the path to the file\n",
        "    Output:\n",
        "        corrections_dict(dict): contain the correct words and the misspelled \n",
        "    \"\"\"\n",
        "    file_text = None\n",
        "    corrections_dict = dict()\n",
        "    with open(file_path, 'r') as f:\n",
        "        file_text = f.readlines()\n",
        "\n",
        "    for line in file_text:\n",
        "        corrct, miss_l = line.split(':')\n",
        "\n",
        "        corrections_dict[corrct] = list(map(lambda x:x.strip().lower(), miss_l.split(',')))\n",
        "\n",
        "    return corrections_dict\n",
        "\n",
        "\n",
        "def read_datafile(file_name):\n",
        "    \"\"\"\n",
        "    Read text file and return lower case letter \n",
        "    Input:\n",
        "        file_name: path to the file with data or the dictionary of the words\n",
        "    Output:\n",
        "        textfile: lower case string with the content of the file_name\n",
        "    \"\"\"\n",
        "    \n",
        "    textfile = str()\n",
        "\n",
        "    with open(file_name) as f:\n",
        "        textfile = \"\".join(f.readlines())\n",
        "        \n",
        "    textfile = textfile.lower()\n",
        "\n",
        "    return textfile\n",
        "\n",
        "\n",
        "\n",
        "def process_data(textfile):\n",
        "    \"\"\"\n",
        "    Return list of words in the string \n",
        "    Input: \n",
        "        textfile: lower case string with the content of the file_name\n",
        "    Output: \n",
        "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
        "    \"\"\"\n",
        "    words = [] \n",
        "    \n",
        "    words = re.findall(generate_exp(), textfile)\n",
        "    \n",
        "    return words\n",
        "\n",
        "\n",
        "# Count the words occurance in the Docs\n",
        "def get_count(word_l):\n",
        "    '''\n",
        "    Get the Count for each word in the list\n",
        "    Input:\n",
        "        word_l: a set of words representing the corpus. \n",
        "    Output:\n",
        "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
        "    '''\n",
        "    \n",
        "    word_count_dict = {}\n",
        "\n",
        "    word_count_dict = Counter(word_l)\n",
        "\n",
        "    return word_count_dict\n",
        "\n",
        "\n",
        "# Get the probability of the word in the docs\n",
        "def get_probs(word_count_dict):\n",
        "    '''\n",
        "    Get probability of every word in the dictionary\n",
        "    Input:\n",
        "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
        "    Output:\n",
        "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
        "    '''\n",
        "    probs = {}\n",
        "\n",
        "    total_words = sum(word_count_dict.values())\n",
        "    probs = dict(map(lambda x:(x[0],x[1]/total_words), word_count_dict.items()))\n",
        "    \n",
        "    return probs\n",
        "\n",
        "def get_aphaUnigram(vocab):\n",
        "    \"\"\"\n",
        "    Get the count of each alphabet in the vocab\n",
        "    Input:\n",
        "        vocab(set): set of words in the big data\n",
        "    Output:\n",
        "        alpha_dict(dict): dictionary contain count of each character\n",
        "    \"\"\"\n",
        "    alpha_dict = defaultdict(int)\n",
        "    for word in vocab:\n",
        "\n",
        "        for alpha in word:\n",
        "            alpha_dict[alpha] += 1\n",
        "\n",
        "    return alpha_dict\n",
        "\n",
        "def get_aphaBigram(vocab):\n",
        "    \"\"\"\n",
        "    Get the count of each consecutive alphabet in the vocab\n",
        "    Input:\n",
        "        vocab(set): set of words in the big data\n",
        "    Output:\n",
        "        alpha_bigram(dict): dictionary contain count of each consecutive character\n",
        "    \"\"\"\n",
        "    alpha_bigram = defaultdict(int)\n",
        "\n",
        "    for correct in vocab:\n",
        "        correct = \"#\" + correct\n",
        "        for word in range(len(correct) - 1):\n",
        "            token1 = correct[word]\n",
        "            token2 = correct[word + 1]\n",
        "            alpha_bigram[tuple((token1, token2))] += 1\n",
        "\n",
        "    return alpha_bigram\n",
        "\n",
        "def get_min_word(corrected, wrong_word):\n",
        "    \"\"\"\n",
        "    Get the distance between any 2 words\n",
        "    Input:\n",
        "        corrected(dict): dictionary of words with its probability \n",
        "        wrong_word(str): the word to check the distance from\n",
        "    Output:\n",
        "        selected(str): the word with smallest distance from wrong_word\n",
        "    \"\"\"\n",
        "    selected = str()\n",
        "    min_edits = 100\n",
        "    corrected = dict(reversed(list(sorted(corrected.items(), key=lambda x: x[1]))))\n",
        "    for target, prob in corrected.items():\n",
        "        _, med = min_edit_distance(wrong_word, target) # (will have edits in the comming edits)\n",
        "\n",
        "        if med < min_edits:\n",
        "            selected = target\n",
        "        \n",
        "    return selected"
      ],
      "metadata": {
        "id": "qUPGL4p1av2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## norvig word creation"
      ],
      "metadata": {
        "id": "VZsvaQkAOpVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mJCsNI1H5-B"
      },
      "outputs": [],
      "source": [
        "# delete character from a word\n",
        "def delete_letter(word, verbose=False):\n",
        "    '''\n",
        "    Delete character from word at each position\n",
        "    Input:\n",
        "        word(str): the string/word for which will generate all possible words \n",
        "                in the vocabulary which have 1 missing character\n",
        "    Output:\n",
        "        delete_l(list): a list of all possible strings obtained by deleting 1 character from word\n",
        "    '''\n",
        "    \n",
        "    delete_l = []\n",
        "    split_l = []\n",
        "    \n",
        "    split_l = [(word[:w], word[w:]) for w in range(len(word))]\n",
        "    delete_l = [R + L[1:] for R, L in split_l if L]\n",
        "    \n",
        "\n",
        "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
        "\n",
        "    return delete_l\n",
        "\n",
        "\n",
        "# switch each neighboring characters\n",
        "def switch_letter(word, verbose=False):\n",
        "    '''\n",
        "    Switch any two consecutive charater\n",
        "    Input:\n",
        "        word: the string/word for which will generate all possible words \n",
        "                in the vocabulary which have 1 switch characters\n",
        "     Output:\n",
        "        switches: a list of all possible strings with one adjacent charater switched\n",
        "    ''' \n",
        "    \n",
        "    switch_l = []\n",
        "    split_l = []\n",
        "    \n",
        "    split_l = [(word[:w], word[w:]) for w in range(len(word))]\n",
        "    \n",
        "    switch_l = [R + L[1] + L[0] + L[2:] for R, L in split_l if len(L) > 1]\n",
        "    \n",
        "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
        "\n",
        "    return switch_l\n",
        "\n",
        "\n",
        "\n",
        "# replace a charcter \n",
        "def replace_letter(word, verbose=False):\n",
        "    '''\n",
        "    Replace any two consecutive charater\n",
        "    Input:\n",
        "        word: the string/word for which will generate all possible words \n",
        "                in the vocabulary which have 1 replaced character\n",
        "    Output:\n",
        "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
        "    ''' \n",
        "    \n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    replace_l = []\n",
        "    split_l = []\n",
        "    \n",
        "    split_l = [(word[:w], word[w:]) for w in range(len(word))]\n",
        "    replace_l = [R + l + L[1:] for R, L in split_l for l in letters if L]\n",
        "    replace_set = set(replace_l)\n",
        "    \n",
        "    replace_l = sorted(list(replace_set))\n",
        "    \n",
        "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
        "    \n",
        "    return replace_l\n",
        "\n",
        "# insert character in the word \n",
        "def insert_letter(word, verbose=False):\n",
        "    '''\n",
        "     insert character at each position in the word\n",
        "    Input:\n",
        "        word: the string/word for which will generate all possible words \n",
        "                in the vocabulary which have 1 switch characters\n",
        "    Output:\n",
        "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
        "    ''' \n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    insert_l = []\n",
        "    split_l = []\n",
        "    \n",
        "    split_l = [(word[:w], word[w:]) for w in range(len(word) +  1)]\n",
        "    insert_l = [R + l + L for R, L in split_l for l in letters]\n",
        "    \n",
        "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
        "    \n",
        "    return insert_l\n",
        "\n",
        "\n",
        "# get the minimum distance between two words\n",
        "\n",
        "def edit_one_letter(word, allow_switches = True):\n",
        "    \"\"\"\n",
        "    Generate a list of all possible combination resulted from one edit\n",
        "    Input:\n",
        "        word: the string/word for which we will generate all possible words that are one edit away.\n",
        "    Output:\n",
        "        edit_one_set: a set of words with one possible edit.\n",
        "    \"\"\"\n",
        "    \n",
        "    edit_one_set = list()\n",
        "    \n",
        "    edit_one_set = insert_letter(word) + delete_letter(word) + replace_letter(word)\n",
        "    if allow_switches:\n",
        "        edit_one_set += switch_letter(word)\n",
        "        \n",
        "    while edit_one_set.count(word):\n",
        "        edit_one_set.remove(word)\n",
        "\n",
        "    return set(edit_one_set)\n",
        "\n",
        "\n",
        "def edit_two_letters(word, allow_switches = True):\n",
        "    '''\n",
        "    Generate a list of all possible combination resulted from two edit\n",
        "    Input:\n",
        "        word: the string/word for which we will generate all possible words \n",
        "    Output:\n",
        "        edit_two_set: a set of strings with all possible two edits\n",
        "    '''\n",
        "    \n",
        "    edit_two_set = list()    \n",
        "    \n",
        "    step_one = edit_one_letter(word)\n",
        "\n",
        "    for w in map(lambda x:edit_one_letter(x, allow_switches=allow_switches), step_one):\n",
        "        edit_two_set += list(w)\n",
        "\n",
        "    return set(edit_two_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# norvig new edtits"
      ],
      "metadata": {
        "id": "IkCrnErGO5nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
        "    '''\n",
        "    Get the minimum distance between two words\n",
        "    Input: \n",
        "        source: a string corresponding to the string we are starting with\n",
        "        target: a string corresponding to the string we want to end with\n",
        "        ins_cost: an integer setting the insert cost\n",
        "        del_cost: an integer setting the delete cost\n",
        "        rep_cost: an integer setting the replace cost\n",
        "    Output:\n",
        "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
        "        med: the minimum edit distance (med) required to convert the source string to the target\n",
        "    '''\n",
        "    \n",
        "    m = len(source) \n",
        "    n = len(target) \n",
        "    \n",
        "    D = np.zeros((m+1, n+1), dtype=int) \n",
        "    \n",
        "    D[0,0] = 0\n",
        "    \n",
        "    for row in range(1,m + 1):\n",
        "        D[row,0] = D[row - 1, 0] + del_cost\n",
        "        \n",
        "    \n",
        "    for col in range(1,n + 1):\n",
        "        D[0,col] = D[0,col-1] + ins_cost\n",
        "        \n",
        "    for row in range(1,m + 1): \n",
        "        \n",
        "        \n",
        "        for col in range(1,n + 1):\n",
        "            \n",
        "            \n",
        "            r_cost = rep_cost\n",
        "            \n",
        "            \n",
        "            if source[row - 1] is target[col - 1]:\n",
        "                \n",
        "                r_cost = 0\n",
        "                \n",
        "            D[row,col] = min(D[row - 1, col] + ins_cost,\n",
        "                             D[row, col - 1] + del_cost,\n",
        "                             D[row - 1,col - 1] + r_cost\n",
        "                            )\n",
        "          \n",
        "    med = D[m, n]\n",
        "    \n",
        "    return D, med\n",
        "\n",
        "\n",
        "\n",
        "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
        "    '''\n",
        "    Get dictionary with n-th bigest probability words/candidates \n",
        "    Input: \n",
        "        word: a user entered string to check for suggestions\n",
        "        probs: a dictionary that maps each word to its probability in the corpus\n",
        "        vocab: a set containing all the vocabulary\n",
        "        n: number of possible word corrections we want returned in the dictionary\n",
        "    Output: \n",
        "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
        "    '''\n",
        "    \n",
        "    suggestions = set()\n",
        "    n_best = dict()\n",
        "\n",
        "    if word in vocab:\n",
        "        suggestions.update(set([word]))\n",
        "\n",
        "    else:\n",
        "        suggestions.update(edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(vocab) or set([word]))\n",
        "\n",
        "    \n",
        "    n_best.update(dict(list((s, probs[s]) if s in probs else (s, 0) for s in suggestions)))\n",
        "\n",
        "    n_best = dict(list(reversed(sorted(n_best.items(), key=lambda x:x[1])))[:n])\n",
        "\n",
        "    \n",
        "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
        "\n",
        "    return n_best\n",
        "\n",
        "\n",
        "def get_ngrams(textfile, w1=None, w2=None):\n",
        "    \"\"\"\n",
        "    Get the n-gram for any word between w1 and w2\n",
        "    Input:\n",
        "        textfile(str): string with lower case letters\n",
        "        w1(str): word usually start of the pattern\n",
        "        w2(str): word usually end of the pattern\n",
        "        \n",
        "    Output:\n",
        "        n_grams_dict: dictionary with count of the words that have the pattern\n",
        "    \"\"\"\n",
        "    \n",
        "    n_grams_dict = dict()\n",
        "    \n",
        "    file_pattern = re.findall(generate_exp(w1, w2), textfile)\n",
        "\n",
        "    if not len(file_pattern) and w1 and w2:\n",
        "        file_pattern = re.findall(generate_exp(w1=w1), textfile)\n",
        "\n",
        "    file_pattern = \" \".join(file_pattern).split(\" \")\n",
        "    count_file = get_count(file_pattern)\n",
        "    \n",
        "\n",
        "    count_file.pop(w1) if w1 and w1 in count_file else None\n",
        "    count_file.pop(w2) if w2 and w2 in count_file else None\n",
        "    \n",
        "    n_grams_dict = get_probs(count_file)\n",
        "    \n",
        "    return n_grams_dict\n",
        "    \n",
        "    \n",
        "\n",
        "def get_unigrams(textfile):\n",
        "    \"\"\"\n",
        "    Get the probability for each word\n",
        "    Input:\n",
        "        textfile(str): string with lower case letters\n",
        "    Output:\n",
        "        words_prob(dict): probabilities of all the words\n",
        "    \"\"\"\n",
        "    words_prob = dict()\n",
        "\n",
        "    words_l = process_data(textfile)\n",
        "\n",
        "    words_cont = get_count(words_l)\n",
        "\n",
        "    words_prob = get_probs(words_cont)\n",
        "\n",
        "    return words_prob, set(words_l), words_cont\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHdOQGgvO5ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# noisy channle"
      ],
      "metadata": {
        "id": "4UtdryhJlRH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_ops(correct, noisy, ops):\n",
        "    \"\"\"\n",
        "    Get which characters have a ops done on it\n",
        "    Input:\n",
        "        correct(str): the correct word\n",
        "        noisy(str): the noisy word that have the operation done\n",
        "        ops: the operation\n",
        "    Output:\n",
        "        tupel(c1, c2): tuple of the two characters that have the operation\n",
        "    \"\"\"\n",
        "\n",
        "    min_len = min(len(correct), len(noisy))\n",
        "\n",
        "    for i in range(min_len):\n",
        "\n",
        "        if correct[i] != noisy[i]:\n",
        "            if i == 0:\n",
        "                return ('#', correct[i]) if ops == 'del' else  ('#' , noisy[i])\n",
        "            \n",
        "            return (correct[i - 1], correct[i]) if ops == 'del' else (correct[i - 1], noisy[i])\n",
        "\n",
        "    return (correct[-2], correct[-1]) if ops == 'del' else (correct[-1], noisy[-1])\n"
      ],
      "metadata": {
        "id": "ldFavyl6lQjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_editOps(correct, noisy):\n",
        "    \"\"\"\n",
        "    Get the operation done over the word correct to get the word noisy\n",
        "    Input:\n",
        "        correct(str): the original word\n",
        "        noisy(str): the word with error\n",
        "    Output:\n",
        "        tuple(ops, c1, c2): tuple have the operatoin and the characters that the operation done over it\n",
        "    \"\"\"\n",
        "\n",
        "    if correct == noisy:\n",
        "        return \"same\", (-1, -1)\n",
        "\n",
        "    if len(correct) < len(noisy):\n",
        "        return \"ins\", check_ops(correct, noisy, 'ins')\n",
        "\n",
        "    elif len(correct) > len(noisy):\n",
        "        return \"del\", check_ops(correct, noisy, 'del')\n",
        "\n",
        "    \n",
        "    return tuple(('rep', (noisy[i], correct[i])) for i in range(len(noisy)) if noisy[i] != correct[i])[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "wkdepOoCnVhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_table():\n",
        "    \"\"\"\n",
        "    Build tables for each operation {insertion, deletion, replacement}\n",
        "    Output:\n",
        "        fram(DataFrame): with count for operation that the table have\n",
        "    \"\"\"\n",
        "    alphabit = '#abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    fram = pd.DataFrame(columns=list(alphabit)[1:], index=list(alphabit))\n",
        "    fram.fillna(0, inplace=True)\n",
        "\n",
        "    return fram\n",
        "\n",
        "def save_tables(tables):\n",
        "    \"\"\"\n",
        "    saves the tables in a csv to faster usge\n",
        "    Input:\n",
        "        tables(list): list of data frames that have the oreder of 'insert', 'delete', 'replace'\n",
        "    \"\"\"\n",
        "    \n",
        "    names = ['insert', 'delete', 'replace']\n",
        "\n",
        "    for num in range(len(tables)):\n",
        "        tables[num].to_csv(f\"{names[num]}.csv\")"
      ],
      "metadata": {
        "id": "Z99sdeVFnYaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the misspelled words and its correction as a dictionary\n",
        "misspelling_dict = read_misspelled()\n",
        "\n",
        "# build the three error operation count table\n",
        "InsOps = build_table()\n",
        "DelOps = build_table()\n",
        "repOps = build_table() \n",
        "\n",
        "\n",
        "# fill the tables with the count of operations\n",
        "for correct_word in misspelling_dict:\n",
        "    for missed in misspelling_dict[correct_word]:\n",
        "        \n",
        "        # cleaning the words to easier processing\n",
        "        missed = re.findall('[a-zA-Z]+', missed)\n",
        "\n",
        "        cleaned_word = re.findall('[a-zA-Z]+', correct_word)\n",
        "\n",
        "        # if the missed and cleaned_word not exist we will continue\n",
        "        if not missed:\n",
        "            continue\n",
        "        if not cleaned_word:\n",
        "            break\n",
        "\n",
        "        cleaned_word = cleaned_word[0]\n",
        "        missed = missed[0]\n",
        "\n",
        "        # get the operation and the characters\n",
        "        (ops, (x, y)) = get_editOps(cleaned_word, missed)\n",
        "    \n",
        "        \n",
        "        # store the count of operation\n",
        "        if ops == 'ins':\n",
        "            InsOps[y][x] += 1\n",
        "        elif ops == 'del':\n",
        "            DelOps[y][x] += 1\n",
        "        elif ops == 'rep':\n",
        "            repOps[y][x] += 1\n",
        "\n",
        "# save the tables\n",
        "save_tables([InsOps, DelOps, repOps])"
      ],
      "metadata": {
        "id": "4zwmQHz1nbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count_unialpha(x):\n",
        "    \"\"\"\n",
        "    Get the count of single characters or space\n",
        "    Input:\n",
        "        x(str): characters\n",
        "    Output:\n",
        "        int: of the count of that characters\n",
        "    \"\"\"\n",
        "    if x == '#':\n",
        "        return len(vocab)\n",
        "    \n",
        "    return count_unialpha[x]"
      ],
      "metadata": {
        "id": "Bb9yYgvMVAev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getProbabilityOfWord(word,noisy):\n",
        "    \"\"\"\n",
        "    Get the probability of each operation for each 2 characters\n",
        "    Input:\n",
        "        word(str): the correct word\n",
        "        noisy(str): the noisy word\n",
        "    Output:\n",
        "        probability(float): probability of each operation\n",
        "    \"\"\"\n",
        "\n",
        "    operation, (x, y) = get_editOps(word,noisy)\n",
        "    if operation == 'ins':\n",
        "        probability = InsOps[y][x]/(get_count_unialpha(x) + LABLCE_SMOOTHING)\n",
        "    elif operation == 'del':\n",
        "        probability = DelOps[y][x]/(count_bialpha[x,y] + LABLCE_SMOOTHING)\n",
        "    elif operation == 'rep':\n",
        "        probability = repOps[y][x]/(get_count_unialpha(y) + LABLCE_SMOOTHING)\n",
        "    elif operation == 'same':\n",
        "        probability = 1\n",
        "    return probability"
      ],
      "metadata": {
        "id": "Aqvx9Zr1nfMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noisy(recommended, corrections_word):\n",
        "    \"\"\"\n",
        "    Get the probability of the noist channel model for the 2 words\n",
        "    Input:\n",
        "        recommended(dict): dictionary of each generated word and its probability\n",
        "        corrections_word(str): the noisy word we want to get the correct from\n",
        "    Output:\n",
        "        options(dict): the recommernded words that could have\n",
        "                        the noisy word generated from with probabilities\n",
        "    \"\"\"\n",
        "    options = {} # Dictionary\n",
        "    for w in recommended:\n",
        "        prob_xw = getProbabilityOfWord(w,corrections_word)  # P(x|w)\n",
        "        #prob_w = words_prob[w]  # P(w)\n",
        "        result = prob_xw * recommended[w]      # P(x|w) . P(w)\n",
        "        options[w] = result                  # Store the result in the dict\n",
        "\n",
        "\n",
        "    return options \n",
        "            \n",
        "        "
      ],
      "metadata": {
        "id": "ZkR7_LlOJmRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "mmo5TY0SnIm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get correction using noisy channel model\n",
        "def correction(wrong_word, n=5, is_sentance=False):\n",
        "    \"\"\"\n",
        "    Get the Correction for the wrong_word\n",
        "    Input:\n",
        "        wrong_word(str): the misspelled word\n",
        "        n(int): the number of candidates we want to process\n",
        "        is_sentance: check if we working on sentance of just one word\n",
        "    output:\n",
        "        recommended(str/list): it's the list of corrected word or just the word with the biggest probability\n",
        "    \"\"\"\n",
        "    # get all possible words with n-th heighst probabilities if the word not in the vocab \n",
        "    recommended = get_corrections(wrong_word, words_prob, vocab, n)\n",
        "    # get the noisy channel recommendation for single word\n",
        "    recommended = get_noisy(recommended, wrong_word)\n",
        "    \n",
        "    if is_sentance:\n",
        "        return recommended\n",
        "    \n",
        "    return max(recommended, key=recommended.get)  # Likely word is the word with the maximum probability \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # take the sentance from the user\n",
        "    sentance = input(\"Enter your sentance: \").strip().lower()\n",
        "    \n",
        "    # read the file and return string with lower case letters\n",
        "    text_file = read_datafile(VOCAB_PATH)\n",
        "\n",
        "    # unigram function use other functions to get dictionary of probabilties for all words\n",
        "    # and set of all the vocabulary in the file\n",
        "    words_prob, vocab, WORDS = get_unigrams(text_file)\n",
        "\n",
        "    # noisy channel dictionaries\n",
        "    count_unialpha = get_aphaUnigram(vocab)\n",
        "    count_bialpha = get_aphaBigram(vocab)\n",
        "\n",
        "    # split the input to itrate on\n",
        "    corrections_words = sentance.split(\" \")\n",
        "\n",
        "    # create new list of the sentance with the corrections\n",
        "    new_sentance = [None] * len(corrections_words)\n",
        "\n",
        "    # looping over the sentance\n",
        "    for word in range(len(corrections_words)) :\n",
        "\n",
        "        # get all possible words with n-th heighst probabilities if the word not in the vocab \n",
        "        recommended = correction(corrections_words[word], n=5, is_sentance=True)\n",
        "        \n",
        "        # conditional probability goes from here\n",
        "        ngrams_dict = []\n",
        "        \n",
        "        max_prob = 0\n",
        "        min_edits = 100\n",
        "        selected = str()\n",
        "\n",
        "        # geting bi-gram and tri gram \n",
        "        if word == 0 and len(corrections_words) > 1:\n",
        "            # if the word in the beging of the sentance will get all possible words usually come with the second one\n",
        "            ngrams_dict = get_ngrams(text_file, w2=corrections_words[word + 1])\n",
        "\n",
        "        elif word + 1 == len(corrections_words) and word > 0:\n",
        "            # if the word in the ending of the sentance will get all possible words usually come with before it\n",
        "            ngrams_dict = get_ngrams(text_file, w1=corrections_words[word - 1])\n",
        "\n",
        "        elif word >= 1:\n",
        "            # getting the tri gram\n",
        "            ngrams_dict = get_ngrams(text_file, w1=corrections_words[word - 1],\n",
        "                                    w2=corrections_words[word + 1])\n",
        "        \n",
        "\n",
        "\n",
        "        # muliply the probabilties of the the possible words and the conditional probability words\n",
        "        # it's similar to maximum likelihood\n",
        "        recommended = mul_dict(recommended, ngrams_dict)\n",
        "\n",
        "        selected = max(recommended, key=recommended.get)\n",
        "        \n",
        "        \n",
        "        # selected = get_min_word(recommended)\n",
        "\n",
        "        new_sentance[word] = selected\n",
        "            \n",
        "        \n",
        "\n",
        "    for w in range(len(new_sentance)):\n",
        "        if new_sentance[w] == corrections_words[w]:\n",
        "            print(colored(new_sentance[w], 'green'), end=\" \")\n",
        "\n",
        "        else:\n",
        "            print(colored(new_sentance[w], 'red'), end=\" \")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "_yQhh7Wp95Yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d231a-b95e-49b1-de1b-e55ef783b0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your sentance: Hi whataa ara wou\n",
            "\u001b[32mhi\u001b[0m \u001b[31mwhat\u001b[0m \u001b[31mare\u001b[0m \u001b[31myou\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "y0Ag7gxHUvvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def unit_tests():\n",
        "#     assert correction('speling') == 'spelling'              # insert\n",
        "#     assert correction('korrectud') == 'corrected'           # replace 2\n",
        "#     assert correction('bycycle') == 'bicycle'               # replace\n",
        "#     assert correction('inconvient') == 'inconvenient'       # insert 2\n",
        "#     assert correction('arrainged') == 'arranged'            # delete\n",
        "#     assert correction('peotry') =='poetry'                  # transpose\n",
        "#     assert correction('peotryy') =='poetry'                 # transpose + delete\n",
        "#     assert correction('word') == 'word'                     # known\n",
        "#     assert correction('quintessential') == 'quintessential' # unknown\n",
        "#     assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
        "#     assert Counter(words('This is a test. 123; A TEST this is.')) == (\n",
        "#            Counter({'123': 1, 'a': 2, 'is': 2, 'test': 2, 'this': 2}))\n",
        "#     assert len(WORDS) == 32192\n",
        "#     assert sum(WORDS.values()) == 1115504\n",
        "#     assert WORDS.most_common(10) == [\n",
        "#      ('the', 79808),\n",
        "#      ('of', 40024),\n",
        "#      ('and', 38311),\n",
        "#      ('to', 28765),\n",
        "#      ('in', 22020),\n",
        "#      ('a', 21124),\n",
        "#      ('that', 12512),\n",
        "#      ('he', 12401),\n",
        "#      ('was', 11410),\n",
        "#      ('it', 10681)]\n",
        "#     assert WORDS['the'] == 79808\n",
        "#     assert P('quintessential') == 0\n",
        "#     assert 0.07 < P('the') < 0.08\n",
        "#     return 'unit_tests pass'\n",
        "unknown_words = []\n",
        "def spelltest(tests, verbose=False):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    import time\n",
        "    start = time.clock()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = correction(wrong)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown_words.append(right if right not in WORDS else None)\n",
        "            unknown += (right not in WORDS)\n",
        "            if verbose:\n",
        "                print('correction({}) => {} ({}); expected {} ({})'\n",
        "                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
        "    dt = time.clock() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "    \n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]\n",
        "\n",
        "# print(unit_tests())\n",
        "spelltest(Testset(open('spell-testset1.txt')), True) # Development set\n",
        "spelltest(Testset(open('spell-testset2.txt'))) # Final test set"
      ],
      "metadata": {
        "id": "asMIlz3vUsLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1198a8-63a2-4bba-cdc5-dd3e797391d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correction(contende) => contended (9); expected contented (13)\n",
            "correction(contended) => contended (9); expected contented (13)\n",
            "correction(proplen) => people (899); expected problem (76)\n",
            "correction(guic) => music (56); expected juice (5)\n",
            "correction(jucie) => julie (71); expected juice (5)\n",
            "correction(juse) => just (767); expected juice (5)\n",
            "correction(compair) => complain (14); expected compare (29)\n",
            "correction(transportibility) => transportibility (0); expected transportability (0)\n",
            "correction(miniscule) => miniscule (0); expected minuscule (0)\n",
            "correction(poartry) => party (298); expected poetry (10)\n",
            "correction(stanerdizing) => stanerdizing (0); expected standardizing (0)\n",
            "correction(biscutes) => disputes (27); expected biscuits (8)\n",
            "correction(receite) => receive (95); expected receipt (13)\n",
            "correction(reciet) => recite (4); expected receipt (13)\n",
            "correction(remined) => remained (231); expected remind (9)\n",
            "correction(annt) => anna (294); expected aunt (52)\n",
            "correction(ther) => their (2955); expected there (2972)\n",
            "correction(vistid) => viscid (3); expected visited (28)\n",
            "correction(ment) => men (1145); expected meant (113)\n",
            "correction(desicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessicate) => delicate (54); expected desiccate (0)\n",
            "correction(dessiccate) => dessiccate (0); expected desiccate (0)\n",
            "correction(semetary) => secretary (52); expected cemetery (2)\n",
            "correction(rember) => member (50); expected remember (161)\n",
            "correction(cak) => can (1095); expected cake (6)\n",
            "correction(awfall) => wall (190); expected awful (29)\n",
            "correction(lauf) => law (432); expected laugh (70)\n",
            "correction(diagrammaticaally) => diagrammaticaally (0); expected diagrammatically (0)\n",
            "correction(pomes) => comes (91); expected poems (3)\n",
            "correction(perple) => people (899); expected purple (29)\n",
            "correction(perpul) => peril (7); expected purple (29)\n",
            "correction(hierachial) => hierachial (0); expected hierarchal (0)\n",
            "correction(wonted) => wonted (1); expected wanted (213)\n",
            "correction(planed) => planed (1); expected planned (15)\n",
            "correction(muinets) => mines (22); expected minutes (146)\n",
            "correction(aranging) => arranging (19); expected arrangeing (0)\n",
            "correction(accesing) => accusing (1); expected accessing (0)\n",
            "correction(embaras) => embers (4); expected embarrass (0)\n",
            "correction(embarass) => embarass (0); expected embarrass (0)\n",
            "correction(auxillary) => axillary (32); expected auxiliary (0)\n",
            "correction(carrer) => carrier (2); expected career (39)\n",
            "correction(poame) => some (1536); expected poem (6)\n",
            "correction(liew) => view (179); expected lieu (7)\n",
            "correction(lones) => lines (133); expected loans (13)\n",
            "correction(addresable) => addresable (0); expected addressable (0)\n",
            "correction(centraly) => central (76); expected centrally (0)\n",
            "correction(certans) => certains (1); expected curtains (5)\n",
            "correction(courtens) => courtiers (20); expected curtains (5)\n",
            "correction(curtions) => portions (56); expected curtains (5)\n",
            "correction(adres) => adores (1); expected address (76)\n",
            "correction(superceed) => superseded (9); expected supersede (1)\n",
            "81% of 270 correct (6% unknown) at 21 words per second \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81% of 270 correct (6% unknown) at 21 words per second \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "https://web.stanford.edu/~jurafsky/slp3/B.pdf"
      ],
      "metadata": {
        "id": "__ldBf81BDI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VOMgC_F6jfwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}